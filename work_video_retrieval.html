<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta content="IE=edge" http-equiv="X-UA-Compatible">
  <meta content="width=device-width,initial-scale=1" name="viewport">
  <meta content="description" name="description">
  <meta name="google" content="notranslate" />


  <!-- Disable tap highlight on IE -->
  <meta name="msapplication-tap-highlight" content="no">
  <link href="./assets/favicon.ico" rel="icon">
  <link href="./main.3f6952e4.css" rel="stylesheet">
  <title>Video Event Retrieval for Vietnamese News</title>  

  
<style>
  body {
    background-color: black;
    color: white;
  }

  .custom-header {
    background-color: black;
    color: white;
  }
  table {
    border-collapse: collapse;
    width: 100%;
    border: 1px solid #ddd;
  }

  th, td {
    border: 1px solid #ddd;
    padding: 8px;
    text-align: center;
  }

  th {
    background-color: #22232b;
  }

  


</style>
</head>

<body class="">
<div id="site-border-left"></div>
<div id="site-border-right"></div>
<div id="site-border-top"></div>
<div id="site-border-bottom"></div>

<header> 
  <nav class="navbar  navbar-fixed-top navbar-defaul custom-header">
    <div class="container">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse" aria-expanded="false">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>

      <div class="collapse navbar-collapse" id="navbar-collapse">
        <ul class="nav navbar-nav">
          <li><a href="./index.html" title="" style="color: #fff;">
              <span class="fa-icon">
                <i class="fa fa-hand-peace-o" aria-hidden="true" style="font-size:16px;color:#fff;"></i>
              </span>
              Hello Page</a></li>
          <li><a href="./works.html" title="" style="color: #fff;">
              <span class="fa-icon">
                <i class="fa fa-bar-chart" aria-hidden="true" style="font-size:16px;color:#fff;"></i>
              </span>
              My Works</a></li>
        </ul>

        <ul class="nav navbar-nav navbar-right navbar-small visible-md visible-lg">
          <li><a href="./work_video_retrieval.html" class="active" style="color: #fff;">001</a></li>
          <li><a href="./work_vietnamese_speaker_regconition.html" style="color: #fff;">002</a></li>
          <li><a href="./work_scientific_abstract_classification.html" style="color: #fff;">003</a></li>
          <li><a href="./work_dog_breed.html" style="color: #fff;">004</a></li>
          <li><a href="./work_landmark.html" style="color: #fff;">005</a></li>
          <li><a href="./work_nyc_311_analysis.html" style="color: #fff;">006</a></li>
          <li><a href="./work_boston_regression.html" style="color: #fff;">007</a></li>
          <li><a href="./work_customer_testimonial.html" style="color: #fff;">008</a></li>
        </ul>

      </div> 
    </div>
  </nav>
</header>

<div class="section-container custom-header">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <img src="./assets/images/video_retrieval/background.jpg" class="img-responsive" alt="">
        <div class="card-container custom-header" style="background-color: #22232b ;opacity: 1; ">
          <div class="text-center">
            <h1 class="h2 custom-header" style="background-color: transparent;">001 : Video Event Retrieval for Vietnamese News</h1>
          </div>
          <p class="text-center">
            I developed this project while participating in the AI-Challenge competition,
            where I utilized cutting-edge AI technologies, including CLIP models and cosine similarity calculations, to retrieve event-specific videos from visual data based on Textual KIS queries.
          </p>
        </div>
      </div>     
      <div class="col-xs-12">
        <div class="row">
          <class="col-xs-12">
            <hr style="border: 0; height: 1px; background-image: linear-gradient(to right, rgba(255, 255, 255, 0), rgba(255, 255, 255, 0.75), rgba(255, 255, 255, 0));">
            <h3 class="h3 custom-header text-center">
              <font color="#59aeba">Project Color Palette</font>
            </h3>
            <hr style="border: 0; height: 1px; background-image: linear-gradient(to right, rgba(255, 255, 255, 0), rgba(255, 255, 255, 0.75), rgba(255, 255, 255, 0));">
            <img src="./assets/images/video_retrieval/custom_palette.png" class="img-responsive" style="display: block; margin: 0 auto;">
            <hr style="border: 0; height: 1px; background-image: linear-gradient(to right, rgba(255, 255, 255, 0), rgba(255, 255, 255, 0.75), rgba(255, 255, 255, 0));">
            <h3 class="h3 custom-header text-center">
              <font color="#59aeba">Model Framework</font>
            </h3>
            <hr style="border: 0; height: 1px; background-image: linear-gradient(to right, rgba(255, 255, 255, 0), rgba(255, 255, 255, 0.75), rgba(255, 255, 255, 0));">
            <img src="./assets/images/video_retrieval/model.png" class="img-responsive" style="display: block; margin: 0 auto;">
            <p align=center><i>Summary of the CLIP Model in the paper <a href="https://arxiv.org/abs/2103.00020" style="color: #f38f66;"><strong>Learning Transferable Visual Models From Natural Language Supervision</strong></a>
            </strong></i></p>
            <p align="justify">
                <br>
                <font size="3" color="#f38f66"><strong>Model Framework for Zero-Shot Video Event Retrieval:</strong></font><br>
                <ul style="list-style-type:number" align="justify">
                  <li><strong>Data Retrieval and Preprocessing:</strong>
                    <ul>
                      <li>Obtain video keyframes organized into subdirectories, representing different videos.</li>
                      <li>Prepare the textual query for the video event retrieval task.</li>
                    </ul>
                  </li>
                  <li><strong>Feature Extraction using CLIP:</strong>
                    <ul>
                      <li>Utilize the pre-trained CLIP model (<code><strong>`openai/clip-vit-base-patch32`</strong></code>) for extracting features from both textual queries and video keyframes.</li>
                      <li>For each keyframe:<br>
                        - Load the image.<br>
                        - Extract image features using CLIP and convert them into numerical representations.</li>
                    </ul>
                  </li>
                  <li><strong>Cosine Similarity Calculation:</strong>
                    <ul>
                      <li>Compute cosine similarity between the extracted features of the textual query and the features of each keyframe.</li>
                      <li>Determine the most similar keyframes based on their similarity scores.</li>
                    </ul>
                  </li>
                  <li><strong>Zero-Shot Inference Process:</strong>
                    <ul>
                      <li>Perform zero-shot inference by inferring associations between textual queries and video keyframes without specific training on these pairs.</li>
                      <li>Leverage the pre-trained CLIP model's generalization capabilities learned from a diverse dataset of images and text.</li>
                    </ul>
                  </li>
                  <li><strong>Result Retrieval and Video Clipping:</strong>
                    <ul>
                      <li>Retrieve the best matching keyframe based on highest similarity score.</li>
                      <li>Extract a segment of the video around the identified keyframe to create a video clip that represents the sought-after event.</li>
                    </ul>
                  </li>
                  <li><strong>Further Analysis or Processing:</strong>
                    <ul>
                      <li>Optional: Conduct additional analysis or processing on the retrieved video clip or related metadata.</li>
                    </ul>
                  </li>
                </ul>
                <br>
                <font size="3" color="#f38f66"><strong>Key Points:</strong></font><br>
                <ul align="justify">
                  <li><strong>Zero-Shot Inference:</strong> The model performs video event retrieval without direct training on the specific textual queries or video keyframes, relying on the pre-trained CLIP model's ability to understand and generalize from diverse image-text pairs.</li>
                  <li><strong>Semantic Understanding:</strong> The model leverages semantic relationships learned during pre-training to associate textual descriptions with visual content without explicit training on these specific associations.</li>
                  <li><strong>Generalization:</strong> By utilizing the CLIP model's pre-existing knowledge learned from a diverse dataset, the model extends its understanding to perform tasks on previously unseen data.</li>
                </ul>
                This framework highlights the application of zero-shot inference in video event retrieval using the CLIP model, demonstrating its ability to generalize and associate text with visual content without explicit training on the specific query-video pairs.
            </p>

            <hr style="border: 0; height: 1px; background-image: linear-gradient(to right, rgba(255, 255, 255, 0), rgba(255, 255, 255, 0.75), rgba(255, 255, 255, 0));">
            <h3 class="h3 custom-header text-center">
              <font color="#59aeba">Dataset</font>
            </h3>
            <hr style="border: 0; height: 1px; background-image: linear-gradient(to right, rgba(255, 255, 255, 0), rgba(255, 255, 255, 0.75), rgba(255, 255, 255, 0));">
            <p align="justify">
              The dataset used in the Video Event Retrieval application is provided by the <a href="https://aichallenge.hochiminhcity.gov.vn" style="color: #f38f66;">competition organizers</a>, comprising videos related to news bulletins produced by various Vietnamese broadcasters and extracted from YouTube.<br>
              The dataset consists of 20 subdirectories, corresponding to news sources, including 'VTV 60 Seconds' program and several others. <br>
              The varying number of videos in each subdirectory showcases the diversity of the data sources, with the duration of each video ranging from 15 to 30 minutes.<br>
              Each subdirectory is accompanied by a number of keyframes, representing iconic images extracted from the respective videos.<br>
              <ul>
                <li>Number of videos: 738 videos (in MP4 format)</li>
                <li>Number of keyframes: 201,745 frames (in JPG format)</li>
              </ul>
              <br>
              Data statistics required for use:
              <table border="1">
                <tr>
                  <th>Folder</th>
                  <th># Videos</th>
                  <th># Keyframes</th>
                </tr>
                <tr>
                  <td>L01</td>
                  <td>31</td>
                  <td>7,658</td>
                </tr>
                <tr>
                  <td>L02</td>
                  <td>30</td>
                  <td>8,820</td>
                </tr>
                <tr>
                  <td>L03</td>
                  <td>31</td>
                  <td>8,554</td>
                </tr>
                <tr>
                  <td>L04</td>
                  <td>31</td>
                  <td>9,908</td>
                </tr>
                <tr>
                  <td>L05</td>
                  <td>28</td>
                  <td>7,781</td>
                </tr>
                <tr>
                  <td>L06</td>
                  <td>28</td>
                  <td>8,980</td>
                </tr>
                <tr>
                  <td>L07</td>
                  <td>31</td>
                  <td>8,694</td>
                </tr>
                <tr>
                  <td>L08</td>
                  <td>29</td>
                  <td>9,779</td>
                </tr>
                <tr>
                  <td>L09</td>
                  <td>30</td>
                  <td>8,074</td>
                </tr>
                <tr>
                  <td>L10</td>
                  <td>30</td>
                  <td>9,058</td>
                </tr>
                <tr>
                  <td>L11</td>
                  <td>31</td>
                  <td>8,631</td>
                </tr>
                <tr>
                  <td>L12</td>
                  <td>31</td>
                  <td>8,294</td>
                </tr>
                <tr>
                  <td>L13</td>
                  <td>30</td>
                  <td>7,865</td>
                </tr>
                <tr>
                  <td>L14</td>
                  <td>30</td>
                  <td>8,835</td>
                </tr>
                <tr>
                  <td>L15</td>
                  <td>31</td>
                  <td>8,404</td>
                </tr>
                <tr>
                  <td>L16</td>
                  <td>31</td>
                  <td>8,914</td>
                </tr>
                <tr>
                  <td>L17</td>
                  <td>33</td>
                  <td>9,260</td>
                </tr>
                <tr>
                  <td>L18</td>
                  <td>23</td>
                  <td>4,961</td>
                </tr>
                <tr>
                  <td>L19</td>
                  <td>99</td>
                  <td>24,245</td>
                </tr>
                <tr>
                  <td>L20</td>
                  <td>100</td>
                  <td>25,030</td>
                </tr>
              </table>
            <br>
            Advantages of the dataset:
            <table border="1">
              <tr>
                <th style="width: 26%;">Advantages</th>
                <th>Impact</th>
              </tr>
              <tr>
                <td>Diversity</td>
                <td>This dataset is diverse in content, covering events both within and outside the country, cultural activities, sports, travel, culinary, fashion, and news alerts.</td>
              </tr>
              <tr>
                <td>Data Size</td>
                <td>The dataset is large with a significant number of videos, providing the model with opportunities to learn from various types of data, improving its understanding and differentiation between contexts.</td>
              </tr>
              <tr>
                <td>Accompanying Keyframes</td>
                <td>The inclusion of accompanying keyframes by the organizers helps in saving time for extracting representative images for videos and content processing.</td>
              </tr>
            </table>
            <br>
            Disadvantages of the dataset:
            <table border="1">
              <tr>
                <th style="width: 26%;">Disadvantages</th>
                <th>Impact</th>
              </tr>
              <tr>
                <td>Image Quality</td>
                <td>Some videos from security cameras have low image quality and lack color information, posing a challenge in extracting keyframes and using the CLIP model.</td>
              </tr>
              <tr>
                <td>Content Diversity</td>
                <td>The diversity of content in news videos poses requirements for diverse training data and the model's capability to understand and differentiate between various content types.</td>
              </tr>
              <tr>
                <td>Data Size</td>
                <td>A large dataset is both an advantage and a disadvantage as it requires significant computational resources.</td>
              </tr>
              <tr>
                <td>Audio and Text Conversion</td>
                <td>One issue with news videos is the amount of accompanying information summarized in the presenter's speech. Processing information from audio and text demands the ability to convert from speech to text.</td>
              </tr>
            </table>
            <br>
            The dataset is a crucial factor for the news video application.
            The diversity of news video content needs to be represented in the dataset to ensure that the model can comprehend and distinguish between various types of videos.<br>
            However, handling videos with low quality and multiple data sources, reliant on computational resources, poses a significant challenge.
            </p>
            
            <br>
            <hr style="border: 0; height: 1px; background-image: linear-gradient(to right, rgba(255, 255, 255, 0), rgba(255, 255, 255, 0.75), rgba(255, 255, 255, 0));">
            
            <h3 class="h3 custom-header text-center">
              <font color="#59aeba">Feature Extraction For Text Query And Keyframes</font>
            </h3>
            <hr style="border: 0; height: 1px; background-image: linear-gradient(to right, rgba(255, 255, 255, 0), rgba(255, 255, 255, 0.75), rgba(255, 255, 255, 0));">
            <p>
            
            </p>
            <br>
            <hr style="border: 0; height: 1px; background-image: linear-gradient(to right, rgba(255, 255, 255, 0), rgba(255, 255, 255, 0.75), rgba(255, 255, 255, 0));">
            <h3 class="h3 custom-header text-center">
              <font color="#59aeba">Cosine Similarity</font>
            </h3>
            <hr style="border: 0; height: 1px; background-image: linear-gradient(to right, rgba(255, 255, 255, 0), rgba(255, 255, 255, 0.75), rgba(255, 255, 255, 0));">
            <p align="justify">
            </p>
            <br>

            <hr style="border: 0; height: 1px; background-image: linear-gradient(to right, rgba(255, 255, 255, 0), rgba(255, 255, 255, 0.75), rgba(255, 255, 255, 0));">
            <h3 class="h3 custom-header text-center">
              <font color="#59aeba">The Limitations Of The Application</font>
            </h3>
            <hr style="border: 0; height: 1px; background-image: linear-gradient(to right, rgba(255, 255, 255, 0), rgba(255, 255, 255, 0.75), rgba(255, 255, 255, 0));">
            <p align="justify">
            </p>

            <br>
            <hr style="border: 0; height: 1px; background-image: linear-gradient(to right, rgba(255, 255, 255, 0), rgba(255, 255, 255, 0.75), rgba(255, 255, 255, 0));">
            <h3 class="h3 custom-header text-center">
              <font color="#59aeba">Deployment Demo</font>
            </h3>
            <hr style="border: 0; height: 1px; background-image: linear-gradient(to right, rgba(255, 255, 255, 0), rgba(255, 255, 255, 0.75), rgba(255, 255, 255, 0));">
            <p align="justify">
              This application interacts with Hadoop to access keyframe and video files from HDFS.<br>
              The keyframe features are stored in a JSON file on HDFS and accessed via HTTP requests.
              Videos are also accessed from HDFS by downloading the video content locally to extract video clips around similar keyframes. <br>
              <br>
              The application interface includes a text box for users to input queries.<br>
              Due to resource limitations and the Vietnamese language processing capability of the CLIP model, queries need to be translated to English before being input into the application for optimal results.<br>
              Besides retrieving the top 10 keyframes with the highest query similarity and a short 10-second clip extracted from the video containing the most similar keyframe,
              the application also features a sidebar providing basic information about the extracted video, such as:<br>
              <ul>
                <li>Author: Video author's name</li>
                <li>Channel_id: YouTube channel name</li>
                <li>Channel_url: Link to the channel where the video was published</li>
                <li>Description: Description of the video on YouTube</li>
                <li>Keywords: Keywords related to the video segment</li>
                <li>Length: Video length in seconds</li>
                <li>Publish_date: Publication date of the video on YouTube</li>
                <li>Thumbnail_url: URL of the video thumbnail</li>
                <li>Title: Title of the video on YouTube</li>
                <li>Watch_url: Link to watch the video on YouTube</li>
              </ul>
              <u>Note</u>: Additional information may not be complete for some videos as the channel author may not display or fill in this information on YouTube.<br>
            </p>
            
            <img src="./assets/images/video_retrieval/demo_video_2.gif" class="img-responsive">           

          </div>
        </div>
      </div>
    </div>
  </div>
</div>


<script>
  document.addEventListener("DOMContentLoaded", function (event) {
     navActivePage();
  });
</script>
<script type="text/javascript" src="./main.70a66962.js"></script>

</body>

</html>