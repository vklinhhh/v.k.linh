<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta content="IE=edge" http-equiv="X-UA-Compatible">
  <meta content="width=device-width,initial-scale=1" name="viewport">
  <meta content="description" name="description">
  <meta name="google" content="notranslate" />


  <!-- Disable tap highlight on IE -->
  <meta name="msapplication-tap-highlight" content="no">
  <link href="./assets/favicon.ico" rel="icon">
  <link href="./main.3f6952e4.css" rel="stylesheet">
  <title>Video Event Retrieval for Vietnamese News</title>  

  
<style>
  body {
    background-color: black;
    color: white;
  }

  .custom-header {
    background-color: black;
    color: white;
  }
  table {
    border-collapse: collapse;
    width: 100%;
    border: 1px solid #ddd;
  }

  th, td {
    border: 1px solid #ddd;
    padding: 8px;
    text-align: center;
  }

  th {
    background-color: #22232b;
  }
  .formula {
    font-size: 18px;
    font-family: 'Courier New', monospace;
    margin-bottom: 20px;
    }
  .custom-blockquote {
    font-size: 1.5em;
    color: #f38f66; 
    border: 2px solid #1e7ca7; /* Add a border */
    padding: 10px; /* Add padding for spacing */
    box-shadow: 0 4px 8px 0 rgba(124,185,144, 0.2);
    transition: box-shadow 0.3s ease;
    text-align: center;
    }

  .custom-blockquote:hover {
      box-shadow: 0 8px 16px 0 rgba(255, 255, 255, 0.2);
      transform: translateY(-3px); 
    }

</style>
</head>

<body class="">
<div id="site-border-left"></div>
<div id="site-border-right"></div>
<div id="site-border-top"></div>
<div id="site-border-bottom"></div>

<header> 
  <nav class="navbar  navbar-fixed-top navbar-defaul custom-header">
    <div class="container">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse" aria-expanded="false">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>

      <div class="collapse navbar-collapse" id="navbar-collapse">
        <ul class="nav navbar-nav">
          <li><a href="./index.html" title="" style="color: #fff;">
              <span class="fa-icon">
                <i class="fa fa-hand-peace-o" aria-hidden="true" style="font-size:16px;color:#fff;"></i>
              </span>
              Hello Page</a></li>
          <li><a href="./works.html" title="" style="color: #fff;">
              <span class="fa-icon">
                <i class="fa fa-bar-chart" aria-hidden="true" style="font-size:16px;color:#fff;"></i>
              </span>
              My Works</a></li>
        </ul>

        <ul class="nav navbar-nav navbar-right navbar-small visible-md visible-lg">
          <li><a href="./work_video_retrieval.html" class="active" style="color: #fff;">001</a></li>
          <li><a href="./work_vietnamese_speaker_regconition.html" style="color: #fff;">002</a></li>
          <li><a href="./work_scientific_abstract_classification.html" style="color: #fff;">003</a></li>
          <li><a href="./work_dog_breed.html" style="color: #fff;">004</a></li>
          <li><a href="./work_boston_regression.html" style="color: #fff;">005</a></li>
          <li><a href="./work_nyc_311_analysis.html" style="color: #fff;">006</a></li>
          <li><a href="./work_landmark.html" style="color: #fff;">007</a></li>
          <li><a href="./work_customer_testimonial.html" style="color: #fff;">008</a></li>
        </ul>

      </div> 
    </div>
  </nav>
</header>

<div class="section-container custom-header">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <img src="./assets/images/video_retrieval/background.jpg" class="img-responsive" alt="">
        <div class="card-container custom-header" style="background-color: #22232b ;opacity: 1; ">
          <div class="text-center">
            <h1 class="h2 custom-header" style="background-color: transparent;">001 : Video Event Retrieval for Vietnamese News</h1>
          </div>
          <p class="text-center">
            I developed this project while participating in the AI-Challenge competition in 2023,
            where I utilized cutting-edge AI technologies, including Zero-Shot learning with CLIP model and cosine similarity calculation, to retrieve event-specific videos from visual data based on Textual KIS queries.
          </p>
        </div>
      </div>     
      <div class="col-xs-12">
        <div class="row">
          <class="col-xs-12">
            <hr style="border: 0; height: 1px; background-image: linear-gradient(to right, rgba(255, 255, 255, 0), rgba(255, 255, 255, 0.75), rgba(255, 255, 255, 0));">
            <h3 class="h3 custom-header text-center">
              <font color="#59aeba">Project Color Palette</font>
            </h3>
            <hr style="border: 0; height: 1px; background-image: linear-gradient(to right, rgba(255, 255, 255, 0), rgba(255, 255, 255, 0.75), rgba(255, 255, 255, 0));">
            <img src="./assets/images/video_retrieval/custom_palette.png" class="img-responsive" style="display: block; margin: 0 auto;">
            <hr style="border: 0; height: 1px; background-image: linear-gradient(to right, rgba(255, 255, 255, 0), rgba(255, 255, 255, 0.75), rgba(255, 255, 255, 0));">
            <h3 class="h3 custom-header text-center">
              <font color="#59aeba">Model Framework</font>
            </h3>
            <hr style="border: 0; height: 1px; background-image: linear-gradient(to right, rgba(255, 255, 255, 0), rgba(255, 255, 255, 0.75), rgba(255, 255, 255, 0));">
            <img src="./assets/images/video_retrieval/model.png" class="img-responsive" style="display: block; margin: 0 auto;">
            <p align=center><i>Summary of the CLIP Model in the paper <a href="https://arxiv.org/abs/2103.00020" style="color: #f38f66;"><strong>Learning Transferable Visual Models From Natural Language Supervision</strong></a>
            </strong></i></p>
            <p align="justify">
                <br>
                <font size="3" color="#f38f66"><strong>Model Framework for Zero-Shot Video Event Retrieval:</strong></font><br>
                <ul style="list-style-type:number" align="justify">
                  <li><strong>Data Retrieval and Preprocessing:</strong>
                    <ul>
                      <li>Obtain video keyframes organized into subdirectories, representing different videos.</li>
                      <li>Prepare the textual query for the video event retrieval task.</li>
                    </ul>
                  </li>
                  <li><strong>Feature Extraction using CLIP:</strong>
                    <ul>
                      <li>Utilize the pre-trained CLIP model (<code><strong>`openai/clip-vit-base-patch32`</strong></code>) for extracting features from both textual queries and video keyframes.</li>
                      <li>For each keyframe:<br>
                        - Load the image.<br>
                        - Extract image features using CLIP and convert them into numerical representations.</li>
                    </ul>
                  </li>
                  <li><strong>Cosine Similarity Calculation:</strong>
                    <ul>
                      <li>Compute cosine similarity between the extracted features of the textual query and the features of each keyframe.</li>
                      <li>Determine the most similar keyframes based on their similarity scores.</li>
                    </ul>
                  </li>
                  <li><strong>Zero-Shot Inference Process:</strong>
                    <ul>
                      <li>Perform zero-shot inference by inferring associations between textual queries and video keyframes without specific training on these pairs.</li>
                      <li>Leverage the pre-trained CLIP model's generalization capabilities learned from a diverse dataset of images and text.</li>
                    </ul>
                  </li>
                  <li><strong>Result Retrieval and Video Clipping:</strong>
                    <ul>
                      <li>Retrieve the best matching keyframe based on highest similarity score.</li>
                      <li>Extract a segment of the video around the identified keyframe to create a video clip that represents the sought-after event.</li>
                    </ul>
                  </li>
                  <li><strong>Further Analysis or Processing:</strong>
                    <ul>
                      <li>Optional: Conduct additional analysis or processing on the retrieved video clip or related metadata.</li>
                    </ul>
                  </li>
                </ul>
                <br>
                <font size="3" color="#f38f66"><strong>Key Points:</strong></font><br>
                <ul align="justify">
                  <li><strong>Zero-Shot Inference:</strong> The model performs video event retrieval without direct training on the specific textual queries or video keyframes, relying on the pre-trained CLIP model's ability to understand and generalize from diverse image-text pairs.</li>
                  <li><strong>Semantic Understanding:</strong> The model leverages semantic relationships learned during pre-training to associate textual descriptions with visual content without explicit training on these specific associations.</li>
                  <li><strong>Generalization:</strong> By utilizing the CLIP model's pre-existing knowledge learned from a diverse dataset, the model extends its understanding to perform tasks on previously unseen data.</li>
                </ul>
                This framework highlights the application of zero-shot inference in video event retrieval using the CLIP model, demonstrating its ability to generalize and associate text with visual content without explicit training on the specific query-video pairs.
            </p>

            <hr style="border: 0; height: 1px; background-image: linear-gradient(to right, rgba(255, 255, 255, 0), rgba(255, 255, 255, 0.75), rgba(255, 255, 255, 0));">
            <h3 class="h3 custom-header text-center">
              <font color="#59aeba">Vietnamese News Dataset</font>
            </h3>
            <hr style="border: 0; height: 1px; background-image: linear-gradient(to right, rgba(255, 255, 255, 0), rgba(255, 255, 255, 0.75), rgba(255, 255, 255, 0));">
            <p align="justify">
              The dataset used in the Video Event Retrieval application is provided by the <a href="https://aichallenge.hochiminhcity.gov.vn" style="color: #f38f66;">competition organizers</a>, comprising videos related to news bulletins produced by various Vietnamese broadcasters and extracted from YouTube.<br>
              The dataset consists of 20 subdirectories, corresponding to news sources, including 'VTV 60 Seconds' program and several others. <br>
              The varying number of videos in each subdirectory showcases the diversity of the data sources, with the duration of each video ranging from 15 to 30 minutes.<br>
              Each subdirectory is accompanied by a number of keyframes, representing iconic images extracted from the respective videos.<br>
              <ul>
                <li>Number of videos: 738 videos (in MP4 format)</li>
                <li>Number of keyframes: 201,745 frames (in JPG format)</li>
              </ul>
              <br>
              Data statistics required for use:
              <table border="1">
                <tr>
                  <th>Folder</th>
                  <th># Videos</th>
                  <th># Keyframes</th>
                </tr>
                <tr>
                  <td>L01</td>
                  <td>31</td>
                  <td>7,658</td>
                </tr>
                <tr>
                  <td>L02</td>
                  <td>30</td>
                  <td>8,820</td>
                </tr>
                <tr>
                  <td>L03</td>
                  <td>31</td>
                  <td>8,554</td>
                </tr>
                <tr>
                  <td>L04</td>
                  <td>31</td>
                  <td>9,908</td>
                </tr>
                <tr>
                  <td>L05</td>
                  <td>28</td>
                  <td>7,781</td>
                </tr>
                <tr>
                  <td>L06</td>
                  <td>28</td>
                  <td>8,980</td>
                </tr>
                <tr>
                  <td>L07</td>
                  <td>31</td>
                  <td>8,694</td>
                </tr>
                <tr>
                  <td>L08</td>
                  <td>29</td>
                  <td>9,779</td>
                </tr>
                <tr>
                  <td>L09</td>
                  <td>30</td>
                  <td>8,074</td>
                </tr>
                <tr>
                  <td>L10</td>
                  <td>30</td>
                  <td>9,058</td>
                </tr>
                <tr>
                  <td>L11</td>
                  <td>31</td>
                  <td>8,631</td>
                </tr>
                <tr>
                  <td>L12</td>
                  <td>31</td>
                  <td>8,294</td>
                </tr>
                <tr>
                  <td>L13</td>
                  <td>30</td>
                  <td>7,865</td>
                </tr>
                <tr>
                  <td>L14</td>
                  <td>30</td>
                  <td>8,835</td>
                </tr>
                <tr>
                  <td>L15</td>
                  <td>31</td>
                  <td>8,404</td>
                </tr>
                <tr>
                  <td>L16</td>
                  <td>31</td>
                  <td>8,914</td>
                </tr>
                <tr>
                  <td>L17</td>
                  <td>33</td>
                  <td>9,260</td>
                </tr>
                <tr>
                  <td>L18</td>
                  <td>23</td>
                  <td>4,961</td>
                </tr>
                <tr>
                  <td>L19</td>
                  <td>99</td>
                  <td>24,245</td>
                </tr>
                <tr>
                  <td>L20</td>
                  <td>100</td>
                  <td>25,030</td>
                </tr>
              </table>
            <br>
            Advantages of the dataset:
            <table border="1">
              <tr>
                <th style="width: 26%;">Advantages</th>
                <th>Impact</th>
              </tr>
              <tr>
                <td>Diversity</td>
                <td>This dataset is diverse in content, covering events both within and outside the country, cultural activities, sports, travel, culinary, fashion, and news alerts.</td>
              </tr>
              <tr>
                <td>Data Size</td>
                <td>The dataset is large with a significant number of videos, providing the model with opportunities to learn from various types of data, improving its understanding and differentiation between contexts.</td>
              </tr>
              <tr>
                <td>Accompanying Keyframes</td>
                <td>The inclusion of accompanying keyframes by the organizers helps in saving time for extracting representative images for videos and content processing.</td>
              </tr>
            </table>
            <br>
            Disadvantages of the dataset:
            <table border="1">
              <tr>
                <th style="width: 26%;">Disadvantages</th>
                <th>Impact</th>
              </tr>
              <tr>
                <td>Image Quality</td>
                <td>Some videos from security cameras have low image quality and lack color information, posing a challenge in extracting keyframes and using the CLIP model.</td>
              </tr>
              <tr>
                <td>Content Diversity</td>
                <td>The diversity of content in news videos poses requirements for diverse training data and the model's capability to understand and differentiate between various content types.</td>
              </tr>
              <tr>
                <td>Data Size</td>
                <td>A large dataset is both an advantage and a disadvantage as it requires significant computational resources.</td>
              </tr>
              <tr>
                <td>Audio and Text Conversion</td>
                <td>One issue with news videos is the amount of accompanying information summarized in the presenter's speech. Processing information from audio and text demands the ability to convert from speech to text.</td>
              </tr>
            </table>
            <br>
            The dataset is a crucial factor for the news video application.
            The diversity of news video content needs to be represented in the dataset to ensure that the model can comprehend and distinguish between various types of videos.<br>
            However, handling videos with low quality and multiple data sources, reliant on computational resources, poses a significant challenge.
            </p>
            
            <br>
            <hr style="border: 0; height: 1px; background-image: linear-gradient(to right, rgba(255, 255, 255, 0), rgba(255, 255, 255, 0.75), rgba(255, 255, 255, 0));">
            
            <h3 class="h3 custom-header text-center">
              <font color="#59aeba">Feature Extraction For Text Query And Keyframes</font>
            </h3>
            <hr style="border: 0; height: 1px; background-image: linear-gradient(to right, rgba(255, 255, 255, 0), rgba(255, 255, 255, 0.75), rgba(255, 255, 255, 0));">
            <p>
              <font size="3" color="#f38f66"><strong>Keyframe feature extraction:</strong></font><br>
              Keyframe feature extraction is a crucial step in video processing and can facilitate concise and effective representations for image-based tasks within videos.<br>

              The process of extracting keyframe features involves the following steps:<br>
              <ul>
                <li>Prepare the CLIP model: Initially, I load the CLIP model and necessary components such as the processor.</li>
                <li>Identify the directory containing keyframes: Determine the path to the directory containing keyframes extracted from the video.</li>
                <li>Retrieve a list of video-specific subdirectories: Gather all subdirectories containing keyframes corresponding to each video.</li>
                <li>Extract features from each keyframe: For each keyframe in every video, I undertake the following steps:<br>
                  - Open the keyframe image using the PIL library.<br>
                  - Encode the keyframe image using the CLIP model to obtain its feature vector.<br>
                  - Store the keyframe features and associate them with the relative path of the keyframe within each video.</li>
                <li>Store the features into a JSON file: Finally, after extracting features from all keyframes, I store them in a JSON file for further data processing purposes.</li>
              </ul>
              <br>
              <font size="3" color="#f38f66"><strong>Text feature extraction:</strong></font><br>
              When a user inputs a text query and initiates the search process, the application utilizes the CLIP model to extract text features from the query.
              This process involves the following steps:
              <ul>
                <li>Encode the Query: The text query gets encoded using the CLIP model employing a text encoder. This transforms the query into a vector representation based on its content.</li>
                <li>Extract Text Features: After encoding, the model extracts text features from the vector representation. These features typically have a dimensionality of 512.</li>
              </ul>
            </p>
            <br>
            <hr style="border: 0; height: 1px; background-image: linear-gradient(to right, rgba(255, 255, 255, 0), rgba(255, 255, 255, 0.75), rgba(255, 255, 255, 0));">
            <h3 class="h3 custom-header text-center">
              <font color="#59aeba">Cosine Similarity</font>
            </h3>
            <hr style="border: 0; height: 1px; background-image: linear-gradient(to right, rgba(255, 255, 255, 0), rgba(255, 255, 255, 0.75), rgba(255, 255, 255, 0));">
            <p align="justify">
            After obtaining both the text features from the query and the keyframe features from the video, the comparison process is performed by calculating the cosine similarity. <br>
            Cosine similarity is a measurement of similarity between two vectors in a multi-dimensional space. It's commonly used in various statistical, machine learning, natural language processing, and other fields for comparing data points.<br>
            <div class="formula">
              <blockquote class="custom-blockquote">
                <span>cos(θ) = </span>
                <span>&Sigma; A<sub>i</sub> * B<sub>i</sub> / (||A|| * ||B||)</span>
              </blockquote>
            </div>
            In which:
            <ul>
              <li>cos(θ) is the cosine similarity.</li>
              <li>A<sub>i</sub> and B<sub>i</sub> are elements of vectors A and B respectively.</li>
              <li>&Sigma; denotes the sum of all elements in the vectors.</li>
              <li>||A|| and ||B|| represent the magnitudes (Euclidean norms) of vectors A and B.</li>
            </ul>
            The important characteristics of cosine similarity include:
            <ul>
              <li><strong>Distance Value:</strong><br>
                The result of cosine similarity ranges from -1 to 1. A value of 1 indicates complete similarity between two vectors, 0 indicates no similarity, and -1 represents complete dissimilarity (completely unlike each other).</li>
              <li><strong>Size Independence:</strong><br>
                Cosine similarity is not influenced by the size of vectors. This means it's an appropriate similarity measure when comparing similarity between vectors of different sizes.</li>
              <li><strong>Operates in Multidimensional Space:</strong><br>
                Cosine similarity works effectively in multidimensional space, where each dimension represents a distinct attribute of the data.</li>
              <li><strong>Commonly Used in Comparison Tasks:</strong><br>
                Cosine similarity is frequently employed in comparison tasks, such as comparing similarity between texts, images, sounds, or data features.</li>
            </ul>
            After the extraction and comparison process, the CLIP model will generate a cosine similarity score for each keyframe concerning the query.
            These scores represent the level of similarity between the keyframe and the text query.<br>
            The result is a list of keyframes sorted in descending order based on their similarity scores.<br>
            Finally, upon obtaining the keyframe most similar to the text query, the application will access the video database on HDFS to extract a 10-second video clip based on the index of the keyframe.<br>
            <br>
            </p>
            <hr style="border: 0; height: 1px; background-image: linear-gradient(to right, rgba(255, 255, 255, 0), rgba(255, 255, 255, 0.75), rgba(255, 255, 255, 0));">
            <h3 class="h3 custom-header text-center">
              <font color="#59aeba">The Limitations Of The Application</font>
            </h3>
            <hr style="border: 0; height: 1px; background-image: linear-gradient(to right, rgba(255, 255, 255, 0), rgba(255, 255, 255, 0.75), rgba(255, 255, 255, 0));">
            <p align="justify">
              The Video Event Retrieval application has made progress in searching and retrieving events in videos based on text queries.<br>
              HoIver, it faces several limitations and unachieved capabilities due to computational resource constraints and specific characteristics of the CLIP model:<br>
              <ul style="list-style-type:number">
                <li><strong>Accuracy in Extracting Information from Home Security Videos:</strong></li>
                    The CLIP model might encounter reduced effectiveness when applied to low-quality home security videos recorded in black and white with low light conditions. The accuracy of image recognition in such environments is a challenge, impacting the efficiency of event retrieval and reducing accuracy.<br>
                <li><strong>Efficiency in Searching Information Not Present in Keyframes:</strong></li>
                    Relying primarily on keyframes for search and retrieval might limit the effectiveness if specific user-requested information isn't included in the extracted keyframes. Developing a mechanism to analyze the entire video content instead of solely relying on keyframes may require enhancements in model capability or real-time object classification.
                <li><strong>Ineffective Audio Information Extraction:</strong></li>
                    The application's focus on image and text-based information extraction lacks efficiency in extracting valuable details from video audio, especially in news videos or scenarios with multiple simultaneous events.
                    Extracting and processing audio, along with speech-to-text conversion, could demand additional resources and computation.
                <li><strong>Inaccurate Vietnamese Text Extraction and OCR:</strong></li>
                    Challenges persist in extracting Vietnamese text and recognizing it accurately within keyframes due to CLIP's primarily English focus. Limited understanding and search capability for Vietnamese text in queries might hinder effective searches in a Vietnamese context, requiring optimization and improvements in Vietnamese text recognition and OCR techniques.
                <li><strong>Slow Computation Time and Application Response:</strong></li>
                    The application's computational inefficiency, particularly due to the extensive dataset and model limitations, leads to slow response times, taking approximately 5-10 minutes for a single query result, affecting competition participation with time constraints.
              </ul> <br>
              These challenges highlight the need for improvements in extracting information from specific video types, enhancing language understanding and OCR for better accuracy, and optimizing computational efficiency for quicker responses.
              
            </p>

            <br>
            <hr style="border: 0; height: 1px; background-image: linear-gradient(to right, rgba(255, 255, 255, 0), rgba(255, 255, 255, 0.75), rgba(255, 255, 255, 0));">
            <h3 class="h3 custom-header text-center">
              <font color="#59aeba">Deployment Demo</font>
            </h3>
            <hr style="border: 0; height: 1px; background-image: linear-gradient(to right, rgba(255, 255, 255, 0), rgba(255, 255, 255, 0.75), rgba(255, 255, 255, 0));">
            <p align="justify">
              This application interacts with Hadoop to access keyframe and video files from HDFS.<br>
              The keyframe features are stored in a JSON file on HDFS and accessed via HTTP requests.
              Videos are also accessed from HDFS by downloading the video content locally to extract video clips around similar keyframes. <br>
              <br>
              The application interface includes a text box for users to input queries.<br>
              Due to resource limitations and the Vietnamese language processing capability of the CLIP model, queries need to be translated to English before being input into the application for optimal results.<br>
              Besides retrieving the top 10 keyframes with the highest query similarity and a short 10-second clip extracted from the video containing the most similar keyframe,
              the application also features a sidebar providing basic information about the extracted video, such as:<br>
              <ul>
                <li>Author: Video author's name</li>
                <li>Channel_id: YouTube channel name</li>
                <li>Channel_url: Link to the channel where the video was published</li>
                <li>Description: Description of the video on YouTube</li>
                <li>Keywords: Keywords related to the video segment</li>
                <li>Length: Video length in seconds</li>
                <li>Publish_date: Publication date of the video on YouTube</li>
                <li>Thumbnail_url: URL of the video thumbnail</li>
                <li>Title: Title of the video on YouTube</li>
                <li>Watch_url: Link to watch the video on YouTube</li>
              </ul>
              <u>Note</u>: Additional information may not be complete for some videos as the channel author may not display or fill in this information on YouTube.<br>
            </p>
            <img src="./assets/images/video_retrieval/ui_app.png" class="img-responsive">           
            <img src="./assets/images/video_retrieval/demo_video_1.gif" class="img-responsive">           
            <img src="./assets/images/video_retrieval/demo_video_2.gif" class="img-responsive">           

          </div>
        </div>
      </div>
    </div>
  </div>
</div>


<script>
  document.addEventListener("DOMContentLoaded", function (event) {
     navActivePage();
  });
</script>
<script type="text/javascript" src="./main.70a66962.js"></script>

</body>

</html>